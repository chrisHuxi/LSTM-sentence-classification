{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#=====================\u52a0\u8f7d\u8981\u7528\u7684\u6a21\u5757======================#\n",
      "from __future__ import print_function\n",
      "#\u4fdd\u8bc1python3.x\u7684\u517c\u5bb9\u6027\n",
      "#import os\n",
      "import numpy as np\n",
      "#import random\n",
      "#import string\n",
      "import tensorflow as tf\n",
      "#\u5bfc\u5165\u753b\u56fe\u6a21\u5757pyplot\n",
      "import matplotlib.pyplot as plt\n",
      "#=====================\u52a0\u8f7d\u8981\u7528\u7684\u6a21\u5757======================#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
        "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#=============\u8bfb\u5165\u6570\u636e===============#\n",
      "####\u5c06\u53e5\u5b50\u7684\u7d22\u5f15\u5f62\u5f0f\u8bfb\u5165\u4e3anp.array\u7684\u5f62\u5f0f####\n",
      "#\u8f93\u5165\uff1a\u53e5\u5b50\u7d22\u5f15\u7684\u6587\u4ef6\u540d\uff08\u8def\u5f84+\u6587\u4ef6\u540d\uff09\uff0c\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\uff08int\uff09\uff0c\u53e5\u5b50\u6700\u5927\u957f\u5ea6\u4f7f\u80fd\uff08bool\uff09\n",
      "#\u8f93\u51fa\uff1a\u6269\u5145\u6216\u526a\u6389\u7684\u53e5\u5b50\u7d22\u5f15\u6570\u5217\uff08np.array\uff09\n",
      "def readSentence2Array(sentenceFileName,inputMaxLength = 128 , inputMaxLengthFlag = True):\n",
      "    file = open(sentenceFileName,'r')\n",
      "    lines = file.readlines()\n",
      "    sentenceList = []\n",
      "    for line in lines:\n",
      "        splitList = line.strip().split(' ')\n",
      "        sentenceList.append(splitList)\n",
      "    maxLength = len(sentenceList[0])\n",
      "    for row in sentenceList:\n",
      "        if maxLength <= len(row):\n",
      "            maxLength = len(row)\n",
      "    if inputMaxLengthFlag == False: \n",
      "        for row in sentenceList:\n",
      "            for i in range(len(row),maxLength):\n",
      "                row.append('0')\n",
      "                \n",
      "    elif inputMaxLengthFlag == True:\n",
      "        temp_list = []\n",
      "        for row in sentenceList:\n",
      "            if len(row) < inputMaxLength:\n",
      "                for i in range(len(row),inputMaxLength):\n",
      "                    row.append('0')\n",
      "                temp_list.append(row)\n",
      "            elif len(row) >= inputMaxLength:\n",
      "                temp_list.append(row[0:inputMaxLength])\n",
      "        sentenceList = temp_list\n",
      "        \n",
      "    numSentenceList = strList2numList2D(sentenceList)\n",
      "    print (np.array(numSentenceList))\n",
      "    return np.array(numSentenceList)\n",
      "\n",
      "####\u5c06\u5b57\u7b26\u7c7b\u578b\u7684list\u8f6c\u6362\u4e3a\u6570\u5b57\u7c7b\u578b\u7684list\uff082D\uff09####\n",
      "#\u8f93\u5165\uff1a\u5b57\u7b26\u7c7b\u578b\u7684list\n",
      "#\u8f93\u51fa\uff1a\u5bf9\u5e94\u7684\u6570\u5b57\u7c7b\u578b\u7684list\n",
      "def strList2numList2D(strlist):\n",
      "    copyList = []\n",
      "    for sentence in strlist:\n",
      "        temp_list = []\n",
      "        for number in sentence:\n",
      "            temp_list.append(int(number))\n",
      "        copyList.append(temp_list)\n",
      "    return copyList\n",
      "\n",
      "####\u8bfb\u5165label\u6587\u4ef6####\n",
      "#\u8f93\u5165\uff1alabel\u6587\u4ef6\u7684\u6587\u4ef6\u540d\n",
      "#\u8f93\u51fa\uff1alabel\u6570\u5217\uff08np.array\uff09\n",
      "def readLabelFile2Array(LabelFileName):\n",
      "    file = open(LabelFileName,'r')\n",
      "    lines = file.readlines()\n",
      "    labelList = []\n",
      "    for line in lines:\n",
      "        splitList = line.strip()\n",
      "        labelList.append(splitList)\n",
      "    return np.array(labelList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\u52a0\u8f7dembeddings\n",
      "savedArrayFile = r'/media/huxi/\u65b0\u52a0\u53771/NLP_exLab/pythonCode/lstm/embedding_and_data/embeddingVocabulary.npy'\n",
      "wordEmbedding = np.load(savedArrayFile)\n",
      "\n",
      "lengthEmbedding = np.shape(wordEmbedding)[1]#\u8bcd\u5411\u91cf\u957f\u5ea6\n",
      "lengthVocabulary = np.shape(wordEmbedding)[0]#\u8bcd\u5178\u957f\u5ea6\n",
      "\n",
      "\n",
      "sentenceFileName = r'/media/huxi/\u65b0\u52a0\u53771/NLP_exLab/pythonCode/lstm/embedding_and_data/sentence_index.txt'\n",
      "sentenceArray = readSentence2Array(sentenceFileName)\n",
      "num_array = np.shape(sentenceArray)[0]#\u53e5\u5b50\u6761\u6570\n",
      "sentenceLength = np.shape(sentenceArray)[1]#\u53e5\u5b50\u957f\u5ea6\n",
      "\n",
      "\n",
      "\n",
      "#\u52a0\u8f7dlabel\u6587\u4ef6\n",
      "LabelFileName =  r'/media/huxi/\u65b0\u52a0\u53771/NLP_exLab/pythonCode/lstm/embedding_and_data/shuffledLabel.txt'\n",
      "labels = readLabelFile2Array(LabelFileName)\n",
      "\n",
      "#\u5c06dataset\u548clabel\u5206\u62108\uff1a1\uff1a1\u7684\u6bd4\u4f8b\n",
      "train_dataset = sentenceArray[0:int(num_array*0.8)]\n",
      "valid_dataset = sentenceArray[int(num_array*0.8):int(num_array*0.8)+int(num_array*0.1)]\n",
      "test_dataset = sentenceArray[int(num_array*0.8)+int(num_array*0.1):]\n",
      "train_labels = labels[0:int(num_array*0.8)]\n",
      "valid_labels = labels[int(num_array*0.8):int(num_array*0.8)+int(num_array*0.1)]\n",
      "test_labels = labels[int(num_array*0.8)+int(num_array*0.1):]\n",
      "\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 9263  9508  2557 ..., 11645 12949  1690]\n",
        " [ 9946   306 11544 ...,     0     0     0]\n",
        " [ 4274  5417  8530 ...,     0     0     0]\n",
        " ..., \n",
        " [ 4274  8764  5013 ...,     0     0     0]\n",
        " [ 5417 10027  6540 ...,     0     0     0]\n",
        " [ 3720  3955 10958 ...,     0     0     0]]\n",
        "Training set (3804, 128) (3804,)\n",
        "Validation set (475, 128) (475,)\n",
        "Test set (476, 128) (476,)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####\u91cd\u65b0\u6784\u9020dataset\u548clabels\u7684\u5f62\u5f0f####\uff08\u5bf9\u4e8eCNN\u9700\u8981\u5c06data\u91cd\u6784\u4e3a\u591a\u4e2a\u901a\u9053\uff0c\u800clstm\u6a21\u578b\u4e0d\u9700\u8981\uff09\n",
      "#\u8f93\u5165\uff1a\u76f4\u63a5\u8bfb\u51fa\u7684dataset\u6570\u5217\uff0c\u76f4\u63a5\u8bfb\u51fa\u7684label\u6570\u5217\n",
      "#\u8f93\u51fa\uff1a\u91cd\u6784\u540e\u7684dataset\u6570\u5217\uff08np.array\uff09\uff0clabel\u6570\u5217\uff08np.array\uff09\n",
      "def reformat(dataset,labels):\n",
      "  kindOfLabels = list(set(labels))\n",
      "  reformedLabels = []\n",
      "  for label in labels:\n",
      "        reformedLabels.append(kindOfLabels.index(label))\n",
      "  reformedLabels = np.array(reformedLabels)\n",
      "  return dataset,reformedLabels\n",
      "    \n",
      "#\u8fd0\u884c\u51fd\u6570\n",
      "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
      "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
      "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set (3804, 128) (3804,)\n",
        "Validation set (475, 128) (475,)\n",
        "Test set (476, 128) (476,)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####\u6839\u636e\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u4e0elabels\u6bd4\u8f83\uff0c\u5f97\u51fa\u51c6\u786e\u7387####\n",
      "#\u8f93\u5165\uff1a\u9884\u6d4b\u7ed3\u679c\uff08one hot array\uff09\uff0clabels\uff08\u4e00\u7ef4 array\uff09\n",
      "#\u8f93\u51fa\uff1a\u51c6\u786e\u7387\n",
      "def accuracy(predictions, labels):\n",
      "  return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
      "          /predictions.shape[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####\u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635####\uff08\u8fd9\u4e2a\u9700\u8981\u6839\u636e\u5177\u4f53\u95ee\u9898\u5b9a\u5236\uff0c\u591a\u5206\u7c7b\u548c\u4e8c\u5206\u7c7b\u95ee\u9898\u4e0d\u540c\uff09\n",
      "#\u8f93\u5165\uff1a\u9884\u6d4b\u7ed3\u679c\uff08one hot array\uff09\uff0clabels\uff08\u4e00\u7ef4 array\uff09\n",
      "#\u8f93\u51fa\uff1a\u6df7\u6dc6\u77e9\u9635\u7684\u56db\u4e2a\u5206\u91cf\u5217\u8868\uff08list\u00d74\uff09\n",
      "def calcuConfusionMatrix(predictions, labels):\n",
      "    truePositiveList = []\n",
      "    trueNegativeList = []\n",
      "    falsePositiveList = []\n",
      "    falseNegativeList = []\n",
      "    print (labels.shape)\n",
      "    for i in range(0,labels.shape[0]):\n",
      "        if np.argmax(predictions, 1)[i] == 1 and labels[i] == 1:\n",
      "            #true positive\n",
      "            truePositiveList.append(i)\n",
      "        elif np.argmax(predictions, 1)[i] == 0 and labels[i] == 0:\n",
      "            trueNegativeList.append(i)\n",
      "        elif np.argmax(predictions, 1)[i] == 1 and labels[i] == 0:\n",
      "            falsePositiveList.append(i)\n",
      "        elif np.argmax(predictions, 1)[i] == 0 and labels[i] == 1:\n",
      "            falseNegativeList.append(i)\n",
      "        else:\n",
      "            print ('something wrong!')\n",
      "    confusionMatrixRow1= [len(truePositiveList),len(falseNegativeList)]\n",
      "    confusionMatrixRow2 = [len(falsePositiveList),len(trueNegativeList)]\n",
      "    confusionMatrix = [(confusionMatrixRow1),(confusionMatrixRow2)]\n",
      "    print ('the Confusion Matrix:')\n",
      "    print (np.mat(confusionMatrix))\n",
      "    return  truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList\n",
      "\n",
      "####\u8ba1\u7b97F1\u503c####\n",
      "#\u8f93\u5165\uff1a\u6df7\u6dc6\u77e9\u9635\u7684\u56db\u4e2a\u5217\u8868\uff08list\u00d74\uff09\n",
      "#\u8f93\u51fa\uff1aF1\u503c  \n",
      "def calcuF1Score(truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList):\n",
      "        precisionScore = len(truePositiveList)/(0.0 + len(truePositiveList) + len(falsePositiveList))\n",
      "        recallScore = len(truePositiveList)/(0.0 + len(truePositiveList) + len(falseNegativeList))\n",
      "        F1Score = 2*len(truePositiveList)/(0.0 + 2*len(truePositiveList) + len(falsePositiveList) + len(falseNegativeList))\n",
      "        return precisionScore,recallScore,F1Score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''==================================================================\n",
      "####LSTM\u6a21\u578b####\n",
      "#\u5728\u6bcf\u4e00\u4e2alstm cell\u4e2d\u7684hidden unit\u7684\u6570\u91cf\n",
      "num_nodes = 64\n",
      "\n",
      "#label\u7684\u7c7b\u522b\u6570\uff0c\u4e5f\u662f\u5206\u7c7b\u6570\n",
      "kindNumLabels = len(list(set(labels)))\n",
      "\n",
      "#\u8bbe\u4e3a\u6700\u5927\u53e5\u5b50\u957f\u5ea6\n",
      "num_unrollings = 128\n",
      "\n",
      "#\u4e00\u4e2abatch\u7684\u5927\u5c0f\n",
      "batch_size = 32\n",
      "=================================================================='''\n",
      "'''\n",
      "wordEmbedding#\u6240\u6709\u8bcd\u5411\u91cf\n",
      "lengthEmbedding#\u8bcd\u5411\u91cf\u957f\u5ea6\n",
      "lengthVocabulary#\u8bcd\u5178\u957f\u5ea6\n",
      "\n",
      "sentenceArray#\u53e5\u5b50\u5411\u91cf\n",
      "num_array#\u53e5\u5b50\u6761\u6570\n",
      "sentenceLength#\u53e5\u5b50\u957f\u5ea6\n",
      "'''\n",
      "\n",
      "'''==================================================================\n",
      "embedding_size = lengthEmbedding#\u8bcd\u5411\u91cf\u7ef4\u6570\n",
      "\n",
      "#\u5b9a\u4e49\u8ba1\u7b97\u56fe\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "  #\u8bcd\u5411\u91cf\u521d\u59cb\u5316\u4e3apre-train\u8fc7\u7684\u8bcd\u5411\u91cf\n",
      "  embeddings = tf.Variable(wordEmbedding)\n",
      "  \n",
      "  # Parameters:\n",
      "  # Input gate: input, previous output, and bias.\n",
      "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Forget gate: input, previous output, and bias.\n",
      "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Memory cell: input, state and bias.                             \n",
      "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Output gate: input, previous output, and bias.\n",
      "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Variables saving state across unrollings.\n",
      "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "  # Classifier weights and biases.\n",
      "  w = tf.Variable(tf.truncated_normal([num_nodes, kindNumLabels], -0.1, 0.1))\n",
      "  b = tf.Variable(tf.zeros([kindNumLabels]))\n",
      "  \n",
      "  # Definition of the cell computation.\n",
      "  def lstm_cell(i, o, state):\n",
      "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
      "    Note that in this formulation, we omit the various connections between the\n",
      "    previous state and the gates.\"\"\"\n",
      "    embed = tf.nn.embedding_lookup(embeddings, i)#1\u00d7embedding_size \u8fd9\u91cc\u7684i\u9700\u8981\u662f\u4e00\u4e2aint32\u6216int64\n",
      "    #print (embed)\n",
      "    \n",
      "    input_gate = tf.sigmoid(tf.matmul(embed, ix) + tf.matmul(o, im) + ib)#i\u4e3a\u8f93\u5165\u7684x,o\u4e3a\u4e0a\u4e00\u4e2acell\u4e2d\u4f20\u6765\u7684h\n",
      "    forget_gate = tf.sigmoid(tf.matmul(embed, fx) + tf.matmul(o, fm) + fb)\n",
      "    update = tf.matmul(embed, cx) + tf.matmul(o, cm) + cb\n",
      "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
      "    output_gate = tf.sigmoid(tf.matmul(embed, ox) + tf.matmul(o, om) + ob)\n",
      "    return output_gate * tf.tanh(state), state\n",
      "\n",
      "  # Input data.\n",
      "  #train_data = list()\n",
      "\n",
      "  train_inputs = list()\n",
      "  for _ in range(num_unrollings):\n",
      "    train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size]))#[batch_size,vocabulary_size]))\n",
      "    \n",
      "  train_labels = tf.placeholder(tf.int64, shape=[batch_size])\n",
      "    \n",
      "  #train_inputs = train_data[:num_unrollings]\n",
      "  #train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
      "  #tf.cast(train_labels,tf.float64)\n",
      "    \n",
      "  train_labels_float = []\n",
      "  for i,_ in enumerate(labels):\n",
      "      #print (type(train_labels[i]))\n",
      "      train_labels_float.append(tf.cast(train_labels[i],tf.float32))\n",
      "      \n",
      "    \n",
      "  # Unrolled LSTM loop.\n",
      "  outputs = list()\n",
      "  output = saved_output\n",
      "  state = saved_state\n",
      "  for i in train_inputs:\n",
      "    output, state = lstm_cell(i, output, state)\n",
      "    outputs.append(output)\n",
      "    \n",
      "  #\u63a5\u4e0b\u6765\u662fpooling\u7684\u8fc7\u7a0b\uff0c\u4f11\u606f\u4e00\u4f1a\u513f\u518d\u505a\u5427\n",
      "  pooling = tf.reduce_mean(tf.pack(outputs), 0)\n",
      "  #pooling\u4e4b\u540e\u5e94\u8be5\u662f\uff08batch_size\uff0cnum_nodes\uff09\n",
      "  \n",
      "  # State saving across unrollings.\n",
      "  with tf.control_dependencies([saved_output.assign(output),\n",
      "                                saved_state.assign(state)]):\n",
      "    # Classifier.\n",
      "    #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "    logits = tf.nn.xw_plus_b(pooling, w, b)\n",
      "    \n",
      "    loss = tf.reduce_mean(\n",
      "      tf.nn.softmax_cross_entropy_with_logits(\n",
      "        logits, tf.one_hot(indices = tf.concat(0, train_labels),depth = kindNumLabels,on_value = 1.0,off_value = 0.0)))\n",
      "  \n",
      "  # Optimizer.\n",
      "  global_step = tf.Variable(0)\n",
      "  learning_rate = tf.train.exponential_decay(\n",
      "    10.0, global_step, 5000, 0.1, staircase=True)\n",
      "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)#\u8fd9\u4e0b\u9762\u7684\u51e0\u884c\u5e72\u5565\u7528\u7684\uff1f\n",
      "  gradients, v = zip(*optimizer.compute_gradients(loss))#\u8fd9\u91cc\u7c7b\u4f3c\u4e8e\u5c06\u4e0d\u540c\u503c\u7684\u68af\u5ea6\u503c\u5168\u90e8unpack\uff0c\u7136\u540e\u5206\u522b\u653e\u5230gradients\u548cv\u4e24\u4e2a\u5217\u8868\u91cc\u9762\n",
      "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)#\u8fd9\u91cc\u662f\u589e\u52a0\u6b63\u5219\u9879\n",
      "  optimizer = optimizer.apply_gradients(\n",
      "    zip(gradients, v), global_step=global_step)\n",
      "\n",
      "  # Predictions.\n",
      "  train_prediction = tf.nn.softmax(logits)\n",
      "  ''''''  \n",
      "  # Sampling and validation eval: batch 1, no unrolling.\n",
      "  #\u4ed4\u7ec6\u60f3\u60f3\u5c31\u8fd9\u4e00\u5757\u770b\u4e0d\u660e\u767d\n",
      "  #https://discussions.udacity.com/t/rnn-lstm-use-implementation/163169/2  \u8d44\u6599\n",
      "  '''\n",
      "'''==================================================================\n",
      "  #\u4e0b\u9762\u8fd9\u4e2a\u662f\u7528\u6765\u8fdb\u884c\u6d4b\u8bd5\u7684\n",
      "  sample_input = tf.placeholder(tf.int64, shape=[1])#[1,vocabulary_size])\n",
      "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  reset_sample_state = tf.group(\n",
      "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
      "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
      "  sample_output, sample_state = lstm_cell(\n",
      "    sample_input, saved_sample_output, saved_sample_state)\n",
      "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
      "                                saved_sample_state.assign(sample_state)]):\n",
      "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
      "=================================================================='''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "'==================================================================\\n  #\\xe4\\xb8\\x8b\\xe9\\x9d\\xa2\\xe8\\xbf\\x99\\xe4\\xb8\\xaa\\xe6\\x98\\xaf\\xe7\\x94\\xa8\\xe6\\x9d\\xa5\\xe8\\xbf\\x9b\\xe8\\xa1\\x8c\\xe6\\xb5\\x8b\\xe8\\xaf\\x95\\xe7\\x9a\\x84\\n  sample_input = tf.placeholder(tf.int64, shape=[1])#[1,vocabulary_size])\\n  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\\n  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\\n  reset_sample_state = tf.group(\\n    saved_sample_output.assign(tf.zeros([1, num_nodes])),\\n    saved_sample_state.assign(tf.zeros([1, num_nodes])))\\n  sample_output, sample_state = lstm_cell(\\n    sample_input, saved_sample_output, saved_sample_state)\\n  with tf.control_dependencies([saved_sample_output.assign(sample_output),\\n                                saved_sample_state.assign(sample_state)]):\\n    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\\n=================================================================='"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####LSTM\u6a21\u578b####\n",
      "#\u5728\u6bcf\u4e00\u4e2alstm cell\u4e2d\u7684hidden unit\u7684\u6570\u91cf\n",
      "num_nodes = 64\n",
      "\n",
      "#label\u7684\u7c7b\u522b\u6570\uff0c\u4e5f\u662f\u5206\u7c7b\u6570\n",
      "kindNumLabels = len(list(set(labels)))\n",
      "\n",
      "#\u8bbe\u4e3a\u6700\u5927\u53e5\u5b50\u957f\u5ea6\n",
      "num_unrollings = 128\n",
      "\n",
      "#\u4e00\u4e2abatch\u7684\u5927\u5c0f\n",
      "batch_size = 32\n",
      "\n",
      "'''\n",
      "wordEmbedding#\u6240\u6709\u8bcd\u5411\u91cf\n",
      "lengthEmbedding#\u8bcd\u5411\u91cf\u957f\u5ea6\n",
      "lengthVocabulary#\u8bcd\u5178\u957f\u5ea6\n",
      "\n",
      "sentenceArray#\u53e5\u5b50\u5411\u91cf\n",
      "num_array#\u53e5\u5b50\u6761\u6570\n",
      "sentenceLength#\u53e5\u5b50\u957f\u5ea6\n",
      "'''\n",
      "\n",
      "\n",
      "embedding_size = lengthEmbedding#\u8bcd\u5411\u91cf\u7ef4\u6570\n",
      "\n",
      "#\u5b9a\u4e49\u8ba1\u7b97\u56fe\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "  #\u8bcd\u5411\u91cf\u521d\u59cb\u5316\u4e3apre-train\u8fc7\u7684\u8bcd\u5411\u91cf\n",
      "  embeddings = tf.Variable(wordEmbedding)\n",
      "  \n",
      "  # Parameters:\n",
      "  # Input gate: input, previous output, and bias.\n",
      "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Forget gate: input, previous output, and bias.\n",
      "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Memory cell: input, state and bias.                             \n",
      "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Output gate: input, previous output, and bias.\n",
      "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
      "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
      "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
      "  # Variables saving state across unrollings.\n",
      "  train_saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "  train_saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
      "    \n",
      "  valid_saved_output = tf.Variable(tf.zeros([valid_dataset.shape[0], num_nodes]), trainable=False)\n",
      "  valid_saved_state = tf.Variable(tf.zeros([valid_dataset.shape[0], num_nodes]), trainable=False)\n",
      "    \n",
      "  test_saved_output = tf.Variable(tf.zeros([test_dataset.shape[0], num_nodes]), trainable=False)\n",
      "  test_saved_state = tf.Variable(tf.zeros([test_dataset.shape[0], num_nodes]), trainable=False)\n",
      "  # Classifier weights and biases.\n",
      "  w = tf.Variable(tf.truncated_normal([num_nodes, kindNumLabels], -0.1, 0.1))\n",
      "  b = tf.Variable(tf.zeros([kindNumLabels]))  \n",
      "    \n",
      "  # Definition of the cell computation.\n",
      "  def lstm_cell(i, o, state):\n",
      "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
      "    Note that in this formulation, we omit the various connections between the\n",
      "    previous state and the gates.\"\"\"\n",
      "    embed = tf.nn.embedding_lookup(embeddings, i)#1\u00d7embedding_size \u8fd9\u91cc\u7684i\u9700\u8981\u662f\u4e00\u4e2aint32\u6216int64\n",
      "    #print (embed)\n",
      "    \n",
      "    input_gate = tf.sigmoid(tf.matmul(embed, ix) + tf.matmul(o, im) + ib)#i\u4e3a\u8f93\u5165\u7684x,o\u4e3a\u4e0a\u4e00\u4e2acell\u4e2d\u4f20\u6765\u7684h\n",
      "    forget_gate = tf.sigmoid(tf.matmul(embed, fx) + tf.matmul(o, fm) + fb)\n",
      "    update = tf.matmul(embed, cx) + tf.matmul(o, cm) + cb\n",
      "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
      "    output_gate = tf.sigmoid(tf.matmul(embed, ox) + tf.matmul(o, om) + ob)\n",
      "    return output_gate * tf.tanh(state), state\n",
      "  '''\n",
      "  #==========\u8bd5\u8bd5\u80fd\u4e0d\u80fd\u5199\u4e2amodel\u51fa\u6765=============\n",
      "  '''\n",
      "  def model(data,chooseState=\"train\"):\n",
      "      # Unrolled LSTM loop.\n",
      "        \n",
      "      outputs = list()\n",
      "      if chooseState==\"train\":\n",
      "            saved_output = train_saved_output\n",
      "            saved_state = train_saved_state\n",
      "      elif chooseState==\"valid\":\n",
      "            saved_output = valid_saved_output\n",
      "            saved_state = valid_saved_state\n",
      "      elif chooseState==\"test\":\n",
      "            saved_output = test_saved_output\n",
      "            saved_state = test_saved_state\n",
      "            \n",
      "      output = saved_output\n",
      "      state = saved_state\n",
      "      for i in data:\n",
      "        output, state = lstm_cell(i, output, state)\n",
      "        outputs.append(output)\n",
      "        \n",
      "      pooling = tf.reduce_mean(tf.pack(outputs), 0)\n",
      "      #pooling\u4e4b\u540e\u5e94\u8be5\u662f\uff08batch_size\uff0cnum_nodes\uff09\n",
      "      if chooseState==\"train\":\n",
      "          # State saving across unrollings.\n",
      "          with tf.control_dependencies([train_saved_output.assign(output),\n",
      "                                        train_saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "            # Classifier.\n",
      "            #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "            logits = tf.nn.xw_plus_b(pooling, w, b)\n",
      "      elif chooseState==\"valid\":\n",
      "          # State saving across unrollings.\n",
      "          with tf.control_dependencies([valid_saved_output.assign(output),\n",
      "                                        valid_saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "            # Classifier.\n",
      "            #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "            logits = tf.nn.xw_plus_b(pooling, w, b)\n",
      "      elif chooseState==\"test\":\n",
      "          # State saving across unrollings.\n",
      "          with tf.control_dependencies([test_saved_output.assign(output),\n",
      "                                        test_saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "            # Classifier.\n",
      "            #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
      "            logits = tf.nn.xw_plus_b(pooling, w, b)\n",
      "      return logits\n",
      "  '''\n",
      "  ==============================================\n",
      "  '''\n",
      "    \n",
      "    \n",
      "  tf_train_inputs = list()\n",
      "  for _ in range(num_unrollings):\n",
      "    tf_train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size]))#[batch_size,vocabulary_size]))\n",
      "    tf_train_inputs\n",
      "  tf_train_labels = tf.placeholder(tf.int64, shape=[batch_size])\n",
      "    \n",
      "  #train_inputs = train_data[:num_unrollings]\n",
      "  #train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
      "  #tf.cast(train_labels,tf.float64)\n",
      "    \n",
      "  tf_train_labels_float = []\n",
      "  for i,_ in enumerate(labels):\n",
      "      #print (type(train_labels[i]))\n",
      "      tf_train_labels_float.append(tf.cast(tf_train_labels[i],tf.float32))\n",
      "        \n",
      "#===============\u5f00\u53d1\u96c6\u6570\u636e\u8f93\u5165==============#\u6709\u7591\u95ee\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01    \n",
      "  tf_valid_dataset = list()\n",
      "  for i in range(num_unrollings):\n",
      "        #tf_valid_dataset.append(tf.constant(valid_dataset[:,i]))\n",
      "        tf_valid_dataset.append(tf.placeholder(tf.int64, shape=[valid_dataset.shape[0]]))\n",
      "#========================================#            \n",
      "        \n",
      "#===============\u6d4b\u8bd5\u96c6\u6570\u636e\u8f93\u5165==============#\u6709\u7591\u95ee\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\n",
      "  tf_test_dataset = list()\n",
      "  for i in range(num_unrollings):\n",
      "        tf_test_dataset.append(tf.placeholder(tf.int64, shape=[test_dataset.shape[0]]))#[batch_size,vocabulary_size]))\n",
      "#========================================#                    \n",
      "\n",
      "        \n",
      "  logits = model(tf_train_inputs)\n",
      "  # State saving across unrollings.\n",
      "  #with tf.control_dependencies([saved_output.assign(output),\n",
      "  #                                 saved_state.assign(state)]):#\u4f7f\u5f97\u8fd9\u4e2a\u8bed\u53e5\u5757\u4e2d\u7684\u8bed\u53e5\u5728saved_output.assign(output),saved_state.assign(state)\u6267\u884c\u4e4b\u540e\u6267\u884c\n",
      "  loss = tf.reduce_mean(\n",
      "      tf.nn.softmax_cross_entropy_with_logits(\n",
      "        logits, tf.one_hot(indices = tf.concat(0, tf_train_labels),depth = kindNumLabels,on_value = 1.0,off_value = 0.0)))\n",
      "  \n",
      "  # Optimizer.\n",
      "  global_step = tf.Variable(0)\n",
      "  learning_rate = tf.train.exponential_decay(\n",
      "    10.0, global_step, 5000, 0.1, staircase=True)\n",
      "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)#\u8fd9\u4e0b\u9762\u7684\u51e0\u884c\u5e72\u5565\u7528\u7684\uff1f\n",
      "  gradients, v = zip(*optimizer.compute_gradients(loss))#\u8fd9\u91cc\u7c7b\u4f3c\u4e8e\u5c06\u4e0d\u540c\u503c\u7684\u68af\u5ea6\u503c\u5168\u90e8unpack\uff0c\u7136\u540e\u5206\u522b\u653e\u5230gradients\u548cv\u4e24\u4e2a\u5217\u8868\u91cc\u9762\n",
      "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)#\u8fd9\u91cc\u662f\u589e\u52a0\u6b63\u5219\u9879\n",
      "  optimizer = optimizer.apply_gradients(\n",
      "    zip(gradients, v), global_step=global_step)\n",
      "\n",
      "  # Predictions.\n",
      "  train_prediction = tf.nn.softmax(logits)\n",
      "    \n",
      "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset,chooseState = \"valid\"))\n",
      "  test_prediction = tf.nn.softmax(model(tf_test_dataset,chooseState = \"test\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_plot_x = []\n",
      "train_plot_y = []\n",
      "valid_plot_y = []\n",
      "\n",
      "num_steps = 1501\n",
      "\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "  tf.initialize_all_variables().run()\n",
      "  print('Initialized')\n",
      "  for step in range(num_steps):\n",
      "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
      "    feed_dict = dict()\n",
      "    feed_dict[tf_train_labels] = batch_labels\n",
      "\n",
      "    \n",
      "    for i in range(num_unrollings):\n",
      "      feed_dict[tf_train_inputs[i]] = batch_data[:,i]   \n",
      "\n",
      "      feed_dict[tf_valid_dataset[i]] = valid_dataset[:,i]           \n",
      "      feed_dict[tf_test_dataset[i]] = test_dataset[:,i]   \n",
      "    _, l, predictions, lr = session.run(\n",
      "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
      "    \n",
      "\n",
      "\n",
      "    if (step % 50 == 0):\n",
      "      print('Minibatch loss at step %d: %f' % (step, l))\n",
      "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
      "      train_plot_x.append(step)\n",
      "      train_plot_y.append(accuracy(predictions, batch_labels))\n",
      "       \n",
      "      print('Validation accuracy: %.1f%%' % accuracy(\n",
      "        valid_prediction.eval(feed_dict = feed_dict), valid_labels))\n",
      "      valid_plot_y.append(accuracy(valid_prediction.eval(feed_dict = feed_dict), valid_labels))\n",
      "       \n",
      "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(feed_dict = feed_dict), test_labels))\n",
      "  truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList = calcuConfusionMatrix(test_prediction.eval(feed_dict = feed_dict), test_labels)\n",
      "  print('precision score: %.1f%%    recall score: %.1f%%    F1 score: %.1f%%' % calcuF1Score( truePositiveList,trueNegativeList,falsePositiveList,falseNegativeList ))\n",
      "  plt.plot(train_plot_x, train_plot_y,'b-',label = \"train accuracy\")\n",
      "  plt.plot(train_plot_x, valid_plot_y,'r-',label = \"valid accuracy\")\n",
      "  plt.xlabel(\"step\")\n",
      "  plt.ylabel(\"accuracy\")\n",
      "  plt.show()\n",
      "''''''  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Minibatch loss at step 0: 0.694709"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 50.0%\n",
        "Validation accuracy: 45.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 50: 1.494342"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 56.2%\n",
        "Validation accuracy: 54.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 100: 1.934679"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 56.2%\n",
        "Validation accuracy: 54.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 150: 3.018538"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 50.0%\n",
        "Validation accuracy: 54.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 200: 0.443675"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 87.5%\n",
        "Validation accuracy: 75.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 250: 1.238570"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 62.5%\n",
        "Validation accuracy: 62.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 300: 0.470673"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 75.0%\n",
        "Validation accuracy: 76.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 350: 0.370535"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 81.2%\n",
        "Validation accuracy: 80.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 400: 0.712002"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 71.9%\n",
        "Validation accuracy: 81.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 450: 0.576747"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 75.0%\n",
        "Validation accuracy: 70.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 500: 0.492169"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 75.0%\n",
        "Validation accuracy: 66.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 550: 0.264608"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 87.5%\n",
        "Validation accuracy: 78.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 600: 0.184400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 93.8%\n",
        "Validation accuracy: 86.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 650: 0.385326"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 78.1%\n",
        "Validation accuracy: 80.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 700: 0.156796"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.9%\n",
        "Validation accuracy: 86.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 750: 0.219425"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 93.8%\n",
        "Validation accuracy: 88.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 800: 0.295435"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 87.5%\n",
        "Validation accuracy: 82.9%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 850: 0.336802"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 90.6%\n",
        "Validation accuracy: 85.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 900: 0.190401"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 90.6%\n",
        "Validation accuracy: 85.7%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 950: 0.073752"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.9%\n",
        "Validation accuracy: 85.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1000: 0.576769"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 75.0%\n",
        "Validation accuracy: 80.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1050: 0.111419"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.9%\n",
        "Validation accuracy: 84.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1100: 0.121710"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 93.8%\n",
        "Validation accuracy: 83.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1150: 0.064458"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 82.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1200: 0.070905"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.9%\n",
        "Validation accuracy: 83.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1250: 0.131504"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 93.8%\n",
        "Validation accuracy: 80.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1300: 0.083847"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 96.9%\n",
        "Validation accuracy: 84.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1350: 0.042950"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 84.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1400: 0.029393"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 83.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1450: 0.042649"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 100.0%\n",
        "Validation accuracy: 84.8%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step 1500: 0.104549"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch accuracy: 93.8%\n",
        "Validation accuracy: 84.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Test accuracy: 84.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(476,)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "the Confusion Matrix:\n",
        "[[231  26]\n",
        " [ 50 169]]\n",
        "precision score: 0.8%    recall score: 0.9%    F1 score: 0.9%\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "''"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}